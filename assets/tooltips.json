{
  "apache-airflow": "<B>Apache Airflow</b><br>Apache Airflow is an open-source platform used to programmatically author, schedule, and monitor workflows. It's essentially a workflow orchestration tool that helps manage and automate complex data pipelines. Airflow uses Python to define workflows as Directed Acyclic Graphs (DAGs), allowing for flexibility and scalability in managing tasks and dependencies. <a href='https://airflow.apache.org/docs/apache-airflow/stable/index.html' target='_blank'>Learn More</a>",
  "agentic-ai": "<b>Agentic AI</b><br>Agentic AI refers to artificial intelligence systems that can independently make decisions, take initiative, and perform tasks with minimal human input—acting more like proactive collaborators than passive tools.",
  "ai-augmentation": "<b>AI Augmentation</b><br/>AI augmentation refers to the use of artificial intelligence to enhance human capabilities and decision-making, rather than replacing human roles entirely. It's about using AI as a tool to improve efficiency, productivity, and creativity, often by automating tasks or providing insights that humans can then use to make better decisions.",
  "astronomer": "<b>Astronomer</b><Br>Astronomer is a managed platform built on Apache Airflow. It helps data teams build, run, and monitor pipelines-as-code at scale—handling infrastructure, security, multi-cloud deployment, and observability. <a href='https://astronomer.io' target='_blank'>Learn More</a>",
  "astronomer-observe": "<b>Astronomer Observe</b><br/>Astronomer Observe is a data orchestration tool that helps manage, monitor, and optimize Apache Airflow pipelines at scale. <a href='https://www.astronomer.io/product/observe/' target='_blank'>Learn More</a>",
  "c4e": "<b>C4E</b><br/>A Center for Enablement (C4E) focuses on empowering teams with reusable assets, frameworks, and support to self-serve and scale delivery. This is a more collaborative standards commitee than a traditional Center of Excellence which is more top-down.",
  "dag": "<b>DAGs</b><br/>In Airflow, a DAG – or a Directed Acyclic Graph – is a collection of all the tasks you want to run, organized in a way that reflects their relationships and dependencies. <a href='https://airflow.apache.org/docs/apache-airflow/1.10.9/concepts.html#dag' target='blank'>Learn More</a>",
  "databricks": "<b>Databricks</b><br/>A unified data and AI platform that combines data engineering, analytics, and machine learning on an open, collaborative lakehouse architecture. <a href='https://www.databricks.com/' target='_blank'>Learn More</a>",
  "dataops-maturity": "<b>DataOps-AI Capability Maturity Model</b><br>Our scale from Ad Hoc to Strategic for data orchestration, governance, quality, access, AI readiness, and business alignment. <a href='/reference/data-ops-ai-maturity-model.html'>Learn more</a>",
  "delta-lake": "<b>Delta Lake</b><br/>An open-source storage framework that enables building a format agnostic Lakehouse architecture with compute engines including Spark, PrestoDB, Flink, Trino, Hive, Snowflake, Google BigQuery, Athena, Redshift, Databricks, Azure Fabric and APIs for Scala, Java, Rust, and Python. <a href='https://delta.io/' target='_blank'>Learn More</a>",
  "kafka": "<b>Kafka</b><br>Apache Kafka is a distributed streaming platform designed to handle large volumes of real-time data. It acts as a central nervous system for data pipelines and streaming applications, enabling high-throughput, fault-tolerant, and scalable data ingestion, storage, and processing. <a href='https://kafka.apache.org/' target='_blank'>Learn More</a>",
  "kinesis": "<b>Kinesis</b><br/>Amazon Kinesis is a suite of services provided by AWS for real-time data streaming and analytics. It allows users to collect, process, and analyze large streams of data in real-time from various sources. <a href='https://aws.amazon.com/kinesis/' target='_blank'>Learn More</a>",
  "mcps": "<b>MCPs (Model Context Protocol)</b><br/>MCP is an open standard developed by Anthropic that allows AI models to access and utilize information from various applications and systems in a standardized way. Essentially, MCP enables AI agents to become more powerful and capable by connecting them to a wider range of resources. <a href='https://www.anthropic.com/news/model-context-protocol' target='_blank'>Learn More</a>",
  "medallion": "<b>Medallion</b><br/>A Databricks Medallion Architecture is a data design pattern used to organize and structure data within a lakehouse, particularly within Databricks. It uses a tiered approach with three layers – Bronze, Silver, and Gold – to progressively improve data quality and usability as it moves through the system. <a href='https://www.databricks.com/glossary/medallion-architecture' target='_blank'>Learn More</a>",
  "openlineage": "<b>OpenLineage</b><br/>Enables consistent collection of lineage metadata, creating a deeper understanding of how data is produced and used. <a href='https://openlineage.io/' target='_blank'>Learn more</a>",
  "spark": "<B>Spark</b><br/>Apache Spark is an open-source, unified analytics engine for large-scale data processing. It's a fast, in-memory processing framework designed for distributed computing, offering high-level APIs in Java, Scala, Python, and R. Spark is used for a wide range of tasks, including batch processing, interactive queries, real-time analytics, machine learning, and graph processing. <a href='https://spark.apache.org/' target='_blank'>Learn More</a>",
  "spark-streaming": "<b>Spark Structured Streaming</b><br/>Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. <a href='https://spark.apache.org/docs/latest/streaming/index.html' target='_blank'>Learn More</a>",
  "unity-catalog": "<b>Unity Catalog</b><br/>A unified governance layer for all data and AI assets in Databricks. <a href='https://www.databricks.com/product/unity-catalog' target='_blank'>Learn more</a>"
}
